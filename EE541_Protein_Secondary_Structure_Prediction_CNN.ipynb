{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"F42EvqeThUhU","executionInfo":{"status":"ok","timestamp":1734026528206,"user_tz":480,"elapsed":24241,"user":{"displayName":"Aditi Surendra Tarate","userId":"05608860842351370008"}}},"outputs":[],"source":["import numpy as np\n","import h5py\n","import torch\n","from torch.utils.data import DataLoader, TensorDataset\n","from torchvision import datasets, transforms\n","import torch.nn as nn\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import confusion_matrix\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.nn.functional as F\n","import os\n","import shutil\n","from sklearn.model_selection import train_test_split\n","from torchvision.datasets import ImageFolder\n","from torchvision.models import resnet34\n","from torch.optim import Adam\n","from torchvision.utils import make_grid\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18745,"status":"ok","timestamp":1733939021996,"user":{"displayName":"Aditi Surendra Tarate","userId":"05608860842351370008"},"user_tz":480},"id":"CyyAsX-1NPHO","outputId":"9638c3ba-988f-48d9-a30e-a968a377dbc9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"xRU28BurhvRN","executionInfo":{"status":"ok","timestamp":1733939023211,"user_tz":480,"elapsed":119,"user":{"displayName":"Aditi Surendra Tarate","userId":"05608860842351370008"}}},"outputs":[],"source":["#[0,22): amino acid residues, with the order of 'A', 'C', 'E', 'D', 'G', 'F', 'I', 'H', 'K', 'M', 'L', 'N', 'Q', 'P', 'S', 'R', 'T', 'W', 'V', 'Y', 'X','NoSeq'\n","#[22,31): Secondary structure labels, with the sequence of 'L', 'B', 'E', 'G', 'I', 'H', 'S', 'T','NoSeq'\n","dataset_path = '/content/drive/MyDrive/Colab Notebooks/EE541_Project/data/cullpdb+profile_6133.npy'\n","sequence_len = 700\n","total_features = 57\n","amino_acid_residues = 21\n","num_classes = 8\n"]},{"cell_type":"markdown","metadata":{"id":"d24o1nF6EWne"},"source":[]},{"cell_type":"code","execution_count":6,"metadata":{"id":"ACIx4D05PRLH","executionInfo":{"status":"ok","timestamp":1733939030046,"user_tz":480,"elapsed":145,"user":{"displayName":"Aditi Surendra Tarate","userId":"05608860842351370008"}}},"outputs":[],"source":["def get_dataset(dataset_path):\n","  ds = np.load(dataset_path)\n","  ds = np.reshape(ds, (ds.shape[0], sequence_len, total_features))\n","  ds_filtered = np.zeros((ds.shape[0], ds.shape[1], amino_acid_residues + num_classes))\n","  ds_filtered[:, :, 0:amino_acid_residues] = ds[:, :, 35:56]\n","  ds_filtered[:, :, amino_acid_residues:] = ds[:, :, amino_acid_residues + 1:amino_acid_residues+ 1 + num_classes]\n","  return ds_filtered"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"9lEUkY1epcFF","executionInfo":{"status":"ok","timestamp":1733939032745,"user_tz":480,"elapsed":126,"user":{"displayName":"Aditi Surendra Tarate","userId":"05608860842351370008"}}},"outputs":[],"source":["def get_data_labels(Dataset):\n","    X = Dataset[:, :, 0:amino_acid_residues]\n","    Y = Dataset[:, :, amino_acid_residues:amino_acid_residues + num_classes]\n","    mask = (np.sum(Y, axis=-1) != 0)  # Mask to ignore padded positions\n","    return X, Y, mask"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"0XAe1P2Y8pXu","executionInfo":{"status":"ok","timestamp":1733939034129,"user_tz":480,"elapsed":128,"user":{"displayName":"Aditi Surendra Tarate","userId":"05608860842351370008"}}},"outputs":[],"source":["# def shuffle_and_split(Dataset, seed=None):\n","#     np.random.seed(seed)\n","#     np.random.shuffle(Dataset)\n","#     train_split = int(Dataset.shape[0]*0.8)\n","#     test_val_split = int(Dataset.shape[0]*0.1)\n","#     Train = Dataset[0:train_split, :, :]\n","#     Test = Dataset[train_split:train_split+test_val_split, :, :]\n","#     Validation = Dataset[train_split+test_val_split:, :, :]\n","#     return Train, Test, Validation\n","\n","def shuffle_and_split(Dataset, seed=None):\n","    train_val, test = train_test_split(Dataset, test_size=0.2, random_state=seed)\n","    train, val = train_test_split(train_val, test_size=0.125, random_state=seed)  # 0.125 * 0.8 = 0.1\n","    return train, test, val"]},{"cell_type":"code","source":["dataset = get_dataset(dataset_path)\n","X, y, mask = get_data_labels(dataset)"],"metadata":{"id":"haZ61GJxDEgD","executionInfo":{"status":"ok","timestamp":1733939070326,"user_tz":480,"elapsed":34441,"user":{"displayName":"Aditi Surendra Tarate","userId":"05608860842351370008"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":929,"status":"ok","timestamp":1733939073109,"user":{"displayName":"Aditi Surendra Tarate","userId":"05608860842351370008"},"user_tz":480},"id":"TQO4Ov26BDOR"},"outputs":[],"source":["X_train, X_test, X_val = shuffle_and_split(X, 100)\n","y_train, y_test, y_val = shuffle_and_split(y, 100)\n","mask_train, mask_test, mask_val = shuffle_and_split(mask, 100)\n","\n","# dataset = get_dataset(dataset_path)\n","\n","# D_train, D_test, D_val = shuffle_and_split(dataset, 100)\n","\n","# X_train, y_train = get_data_labels(D_train)\n","# X_test, y_test = get_data_labels(D_test)\n","# X_val, y_val = get_data_labels(D_val)\n","\n","# print(\"Dataset Loaded\")"]},{"cell_type":"code","source":["X_test.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":144},"id":"V5bopSPsDFr8","executionInfo":{"status":"error","timestamp":1733938775743,"user_tz":480,"elapsed":143,"user":{"displayName":"Aditi Surendra Tarate","userId":"05608860842351370008"}},"outputId":"922a6227-e2a2-4468-b039-701843a19e14"},"execution_count":2,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'X_test' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-cf290153e199>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"]}]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":677,"status":"ok","timestamp":1733939078011,"user":{"displayName":"Aditi Surendra Tarate","userId":"05608860842351370008"},"user_tz":480},"id":"AzYdHHyfCEAw","outputId":"f3022686-7e9b-4205-e65e-8286a3d2b6e6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model Architecture:\n","CNNModel(\n","  (conv1): Conv1d(21, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n","  (dropout1): Dropout(p=0.2, inplace=False)\n","  (conv2): Conv1d(128, 64, kernel_size=(11,), stride=(1,), padding=(5,))\n","  (dropout2): Dropout(p=0.2, inplace=False)\n","  (conv3): Conv1d(64, 8, kernel_size=(11,), stride=(1,), padding=(5,))\n",")\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","# Define hyperparameters\n","LR = 0.001  # Learning rate\n","drop_out = 0.2  # Dropout rate\n","batch_dim = 64  # Batch size\n","nn_epochs = 50  # Number of epochs\n","loss_fn = nn.CrossEntropyLoss()  # Cross-entropy loss function for classification\n","\n","# Convert data to PyTorch tensors\n","X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n","y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n","mask_train_tensor = torch.tensor(mask_train, dtype=torch.bool)\n","\n","X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n","y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n","mask_test_tensor = torch.tensor(mask_test, dtype=torch.bool)\n","\n","X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n","y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n","mask_val_tensor = torch.tensor(mask_val, dtype=torch.bool)\n","\n","# Create DataLoader for batching\n","train_dataset = TensorDataset(X_train_tensor, y_train_tensor, mask_train_tensor)\n","test_dataset = TensorDataset(X_test_tensor, y_test_tensor, mask_test_tensor)\n","val_dataset = TensorDataset(X_val_tensor, y_val_tensor, mask_val_tensor)\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_dim, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=batch_dim, shuffle=False)\n","val_loader = DataLoader(val_dataset, batch_size=batch_dim, shuffle=False)\n","\n","class CNNModel(nn.Module):\n","    def __init__(self, sequence_len, amino_acid_residues, num_classes):\n","        super(CNNModel, self).__init__()\n","        # First 1D Convolutional Layer\n","        self.conv1 = nn.Conv1d(in_channels=amino_acid_residues,\n","                               out_channels=128,\n","                               kernel_size=11,\n","                               padding=5)  # `padding=5` ensures the input and output have the same length\n","        # Dropout after the first Conv layer\n","        self.dropout1 = nn.Dropout(drop_out)\n","\n","        # Second 1D Convolutional Layer\n","        self.conv2 = nn.Conv1d(in_channels=128,\n","                               out_channels=64,\n","                               kernel_size=11,\n","                               padding=5)\n","        # Dropout after the second Conv layer\n","        self.dropout2 = nn.Dropout(drop_out)\n","\n","        # Final 1D Convolutional Layer for classification\n","        self.conv3 = nn.Conv1d(in_channels=64,\n","                               out_channels=num_classes,\n","                               kernel_size=11,\n","                               padding=5)\n","        # Softmax is applied in the forward pass for class probabilities\n","\n","    def forward(self, x):\n","        # Pass through the first Conv layer with ReLU activation\n","        x = F.relu(self.conv1(x))\n","        x = self.dropout1(x)  # Apply dropout\n","\n","        # Pass through the second Conv layer with ReLU activation\n","        x = F.relu(self.conv2(x))\n","        x = self.dropout2(x)  # Apply dropout\n","\n","        # Pass through the final Conv layer and apply softmax along the channel dimension\n","        x = self.conv3(x)\n","        # x = F.softmax(x, dim=1)  # Softmax along the classes (channel dimension)\n","        return x\n","\n","\n","# Instantiate the model\n","sequence_len = 700  # Example sequence length\n","amino_acid_residues = 21  # Number of amino acid residues (input channels)\n","num_classes = 8  # Number of output classes (e.g., Q8 accuracy classification)\n","\n","model = CNNModel(sequence_len, amino_acid_residues, num_classes)\n","\n","# Optimizer setup\n","optimizer = optim.Adam(model.parameters(), lr=LR)\n","\n","# Model Summary\n","print(\"Model Architecture:\")\n","print(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RuwcKvPLFDk9","outputId":"8c08de21-6361-4606-c7b1-4d72208a9ef4","executionInfo":{"status":"ok","timestamp":1733022217720,"user_tz":480,"elapsed":1468506,"user":{"displayName":"Aditi Surendra Tarate","userId":"05608860842351370008"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/20], Loss: 2.0636, Q8 Accuracy: 1.68%\n","Epoch [2/20], Loss: 2.0392, Q8 Accuracy: 21.59%\n","Epoch [3/20], Loss: 2.0277, Q8 Accuracy: 11.46%\n","Epoch [4/20], Loss: 2.0242, Q8 Accuracy: 6.12%\n","Epoch [5/20], Loss: 2.0225, Q8 Accuracy: 5.00%\n","Epoch [6/20], Loss: 2.0215, Q8 Accuracy: 4.93%\n","Epoch [7/20], Loss: 2.0200, Q8 Accuracy: 4.89%\n","Epoch [8/20], Loss: 2.0190, Q8 Accuracy: 4.80%\n","Epoch [9/20], Loss: 2.0184, Q8 Accuracy: 4.73%\n","Epoch [10/20], Loss: 2.0183, Q8 Accuracy: 4.70%\n","Epoch [11/20], Loss: 2.0180, Q8 Accuracy: 4.61%\n","Epoch [12/20], Loss: 2.0176, Q8 Accuracy: 4.56%\n","Epoch [13/20], Loss: 2.0175, Q8 Accuracy: 4.48%\n","Epoch [14/20], Loss: 2.0171, Q8 Accuracy: 4.43%\n","Epoch [15/20], Loss: 2.0171, Q8 Accuracy: 4.38%\n","Epoch [16/20], Loss: 2.0167, Q8 Accuracy: 4.37%\n","Epoch [17/20], Loss: 2.0163, Q8 Accuracy: 4.36%\n","Epoch [18/20], Loss: 2.0160, Q8 Accuracy: 4.34%\n","Epoch [19/20], Loss: 2.0157, Q8 Accuracy: 4.31%\n","Epoch [20/20], Loss: 2.0154, Q8 Accuracy: 4.31%\n","Test Q8 Accuracy: 4.32%\n","Validation Q8 Accuracy: 4.08%\n"]}],"source":["# Training loop\n","for epoch in range(nn_epochs):\n","    model.train()\n","    total_loss = 0\n","    correct_predictions = 0\n","    total_samples = 0\n","\n","    for batch_X, batch_y in train_loader:\n","        # Move to device (GPU/CPU)\n","        batch_X = batch_X.permute(0, 2, 1)  # Change to shape (batch_size, channels, seq_len)\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        outputs = model(batch_X)\n","        _, predicted = torch.max(outputs, dim=1)\n","        #print(predicted.shape)\n","        # Convert batch_y to class indices (argmax across last dimension)\n","        target_class_indices = batch_y.argmax(dim=2)  # Shape: (batch_size, sequence_len)\n","        #print(batch_y.shape)\n","        #print(batch_y.argmax(dim=2).shape)\n","        # Calculate loss and perform backpropagation\n","        # loss = loss_fn(outputs.view(-1, num_classes), batch_y.view(-1).long())\n","        loss = loss_fn(outputs.view(-1, num_classes), target_class_indices.view(-1))\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","        # Track Q8 accuracy\n","        correct_predictions += (predicted == batch_y.argmax(dim=2)).sum().item()\n","        total_samples += batch_X.size(0) * batch_X.size(2)\n","\n","    # Calculate Q8 accuracy for the training set\n","    q8_train_accuracy = 100 * correct_predictions / total_samples\n","\n","    print(f'Epoch [{epoch+1}/{nn_epochs}], Loss: {total_loss/len(train_loader):.4f}, Q8 Accuracy: {q8_train_accuracy:.2f}%')\n","\n","# Evaluation function for test/validation data\n","def evaluate(model, data_loader):\n","    model.eval()\n","    correct_predictions = 0\n","    total_samples = 0\n","    with torch.no_grad():\n","        for batch_X, batch_y in data_loader:\n","            batch_X = batch_X.permute(0, 2, 1)  # Change to shape (batch_size, channels, seq_len)\n","            outputs = model(batch_X)\n","            _, predicted = torch.max(outputs, dim=1)\n","\n","            # Track Q8 accuracy\n","            correct_predictions += (predicted == batch_y.argmax(dim=2)).sum().item()\n","            total_samples += batch_X.size(0) * batch_X.size(2)\n","\n","    q8_accuracy = 100 * correct_predictions / total_samples\n","    return q8_accuracy\n","\n","# Evaluate on test and validation data\n","test_q8_accuracy = evaluate(model, test_loader)\n","val_q8_accuracy = evaluate(model, val_loader)\n","\n","print(f'Test Q8 Accuracy: {test_q8_accuracy:.2f}%')\n","print(f'Validation Q8 Accuracy: {val_q8_accuracy:.2f}%')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"EXkvBVy5NiML","outputId":"d2cad48b-2a0b-4225-acde-67f668581a32"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/20], Loss: 2.0046, Q8 Accuracy: 2.92%\n","Epoch [2/20], Loss: 2.0045, Q8 Accuracy: 2.93%\n","Epoch [3/20], Loss: 2.0041, Q8 Accuracy: 2.91%\n","Epoch [4/20], Loss: 2.0040, Q8 Accuracy: 2.91%\n","Epoch [5/20], Loss: 2.0038, Q8 Accuracy: 2.85%\n","Epoch [6/20], Loss: 2.0040, Q8 Accuracy: 2.88%\n","Epoch [7/20], Loss: 2.0037, Q8 Accuracy: 2.85%\n","Epoch [8/20], Loss: 2.0036, Q8 Accuracy: 2.84%\n","Epoch [9/20], Loss: 2.0035, Q8 Accuracy: 2.82%\n","Epoch [10/20], Loss: 2.0037, Q8 Accuracy: 2.79%\n","Epoch [11/20], Loss: 2.0039, Q8 Accuracy: 2.82%\n","Epoch [12/20], Loss: 2.0036, Q8 Accuracy: 2.79%\n","Epoch [13/20], Loss: 2.0034, Q8 Accuracy: 2.78%\n","Epoch [14/20], Loss: 2.0039, Q8 Accuracy: 2.77%\n","Epoch [15/20], Loss: 2.0035, Q8 Accuracy: 2.71%\n","Epoch [16/20], Loss: 2.0036, Q8 Accuracy: 2.72%\n","Epoch [17/20], Loss: 2.0035, Q8 Accuracy: 2.74%\n","Epoch [18/20], Loss: 2.0035, Q8 Accuracy: 2.72%\n","Epoch [19/20], Loss: 2.0036, Q8 Accuracy: 2.72%\n","Epoch [20/20], Loss: 2.0035, Q8 Accuracy: 2.75%\n","Test Q8 Accuracy: 2.45%\n","Validation Q8 Accuracy: 2.31%\n"]}],"source":["# Training loop\n","for epoch in range(nn_epochs):\n","    model.train()\n","    total_loss = 0\n","    correct_predictions = 0\n","    total_samples = 0\n","\n","    for batch_X, batch_y in train_loader:\n","        # print(f\"batch_X shape: {batch_X.shape}, batch_y shape: {batch_y.shape}\")\n","\n","        # Move to device (GPU/CPU)\n","        batch_X = batch_X.permute(0, 2, 1)  # Change to shape (batch_size, channels, seq_len)\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        outputs = model(batch_X)  # Shape: (batch_size, num_classes, seq_len)\n","        _, predicted = torch.max(outputs, dim=1)  # Shape: (batch_size, seq_len)\n","\n","        # Convert batch_y to class indices (argmax across last dimension)\n","        target_class_indices = batch_y.argmax(dim=2)  # Shape: (batch_size, seq_len)\n","\n","        # Calculate loss and perform backpropagation\n","        loss = loss_fn(outputs.view(-1, num_classes), target_class_indices.view(-1))\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","        # Track Q8 accuracy\n","        correct_predictions += (predicted == target_class_indices).sum().item()\n","        total_samples += batch_X.size(0) * batch_X.size(2)  # batch_size * seq_len\n","\n","    # Calculate Q8 accuracy for the training set\n","    q8_train_accuracy = 100 * correct_predictions / total_samples\n","\n","    print(f'Epoch [{epoch+1}/{nn_epochs}], Loss: {total_loss/len(train_loader):.4f}, Q8 Accuracy: {q8_train_accuracy:.2f}%')\n","\n","# Evaluation function for test/validation data\n","def evaluate(model, data_loader):\n","    model.eval()\n","    correct_predictions = 0\n","    total_samples = 0\n","    with torch.no_grad():\n","        for batch_X, batch_y in data_loader:\n","            batch_X = batch_X.permute(0, 2, 1)  # Change to shape (batch_size, channels, seq_len)\n","            outputs = model(batch_X)  # Shape: (batch_size, num_classes, seq_len)\n","            _, predicted = torch.max(outputs, dim=1)  # Shape: (batch_size, seq_len)\n","\n","            # Convert batch_y to class indices\n","            target_class_indices = batch_y.argmax(dim=2)  # Shape: (batch_size, seq_len)\n","\n","            # Track Q8 accuracy\n","            correct_predictions += (predicted == target_class_indices).sum().item()\n","            total_samples += batch_X.size(0) * batch_X.size(2)\n","\n","    q8_accuracy = 100 * correct_predictions / total_samples\n","    return q8_accuracy\n","\n","# Evaluate on test and validation data\n","test_q8_accuracy = evaluate(model, test_loader)\n","val_q8_accuracy = evaluate(model, val_loader)\n","\n","print(f'Test Q8 Accuracy: {test_q8_accuracy:.2f}%')\n","print(f'Validation Q8 Accuracy: {val_q8_accuracy:.2f}%')"]},{"cell_type":"code","source":["#new code CNN architecture\n","\n","class ProteinCNN(nn.Module):\n","    def __init__(self, amino_acid_residues, num_classes, drop_out=0.1):\n","        super(ProteinCNN, self).__init__()\n","\n","        # First convolutional block\n","        self.conv1 = nn.Conv1d(\n","            in_channels=amino_acid_residues,  # 21 amino acid features\n","            out_channels=256,                # Number of filters\n","            kernel_size=11,                  # Window size\n","            padding=5                        # Same padding to maintain sequence length\n","        )\n","        self.bn1 = nn.BatchNorm1d(256)       # Batch normalization\n","        self.dropout1 = nn.Dropout(drop_out) # Dropout layer\n","\n","        # Second convolutional block\n","        self.conv2 = nn.Conv1d(\n","            in_channels=256,\n","            out_channels=128,\n","            kernel_size=11,\n","            padding=5\n","        )\n","        self.bn2 = nn.BatchNorm1d(128)\n","        self.dropout2 = nn.Dropout(drop_out)\n","\n","        # third convolutional layer\n","        self.conv3 = nn.Conv1d(\n","            in_channels=128,\n","            out_channels=64,\n","            kernel_size=11,\n","            padding=5\n","        )\n","        self.bn3 = nn.BatchNorm1d(64)\n","        self.dropout3 = nn.Dropout(drop_out)\n","\n","        # output convolutional layer\n","        self.conv4 = nn.Conv1d(\n","            in_channels=64,\n","            out_channels=num_classes,\n","            kernel_size=11,\n","            padding=5\n","        )\n","    def forward(self, x):\n","        # Input shape: (batch_size, amino_acid_residues, sequence_len)\n","\n","        # First conv block\n","        x = F.relu(self.bn1(self.conv1(x)))  # Conv -> BatchNorm -> ReLU\n","        x = self.dropout1(x)                # Dropout\n","\n","        # Second conv block\n","        x = F.relu(self.bn2(self.conv2(x))) # Conv -> BatchNorm -> ReLU\n","        x = self.dropout2(x)                # Dropout\n","\n","        # third conv block\n","        x = F.relu(self.bn3(self.conv3(x))) # Conv -> BatchNorm -> ReLU\n","        x = self.dropout3(x)\n","\n","        # Final layer\n","        x = self.conv4(x)                   # No activation; output raw logits\n","        return x  # Shape: (batch_size, num_classes, sequence_len)\n","\n","# Instantiate the model\n","model = ProteinCNN(amino_acid_residues=21, num_classes=8, drop_out=0.1)\n","\n","# Optimizer and Learning Rate\n","optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n","\n","# Print the model summary\n","print(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NaBCT3I9H-nF","executionInfo":{"status":"ok","timestamp":1733039935247,"user_tz":480,"elapsed":148,"user":{"displayName":"Aditi Surendra Tarate","userId":"05608860842351370008"}},"outputId":"5ca86981-d2e2-4606-9207-545769592303"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ProteinCNN(\n","  (conv1): Conv1d(21, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n","  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (dropout1): Dropout(p=0.1, inplace=False)\n","  (conv2): Conv1d(256, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n","  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (dropout2): Dropout(p=0.1, inplace=False)\n","  (conv3): Conv1d(128, 64, kernel_size=(11,), stride=(1,), padding=(5,))\n","  (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (dropout3): Dropout(p=0.1, inplace=False)\n","  (conv4): Conv1d(64, 8, kernel_size=(11,), stride=(1,), padding=(5,))\n",")\n"]}]},{"cell_type":"code","source":["for epoch in range(nn_epochs):\n","    model.train()\n","    total_loss = 0\n","    correct_predictions = 0\n","    total_samples = 0\n","\n","    for batch_X, batch_y in train_loader:\n","        # Reshape input to match Conv1D expectations\n","        batch_X = batch_X.permute(0, 2, 1)  # (batch_size, amino_acid_residues, sequence_len)\n","\n","        # Forward pass\n","        outputs = model(batch_X)  # Shape: (batch_size, num_classes, sequence_len)\n","        outputs = outputs.permute(0, 2, 1)  # (batch_size, sequence_len, num_classes)\n","\n","        # Reshape for loss computation\n","        # Convert batch_y from one-hot encoding to class indices\n","        target_class_indices = batch_y.argmax(dim=2)  # Shape: (batch_size, sequence_len)\n","        loss = loss_fn(outputs.reshape(-1, num_classes), target_class_indices.reshape(-1).long())\n","\n","        # Backpropagation\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","        # Accuracy computation\n","        _, predicted = torch.max(outputs, dim=2)  # Predictions across sequence\n","        correct_predictions += (predicted == batch_y.argmax(dim=2)).sum().item()\n","        total_samples += batch_y.numel()\n","\n","    accuracy = 100 * correct_predictions / total_samples\n","    print(f\"Epoch {epoch+1}/{nn_epochs}, Loss: {total_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":454},"id":"EfFYsjKdN9S0","executionInfo":{"status":"error","timestamp":1733040699865,"user_tz":480,"elapsed":754165,"user":{"displayName":"Aditi Surendra Tarate","userId":"05608860842351370008"}},"outputId":"445f6b3c-f1c0-4a7d-beb2-f8fde7d295e7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50, Loss: 30.9778, Accuracy: 10.87%\n","Epoch 2/50, Loss: 22.3120, Accuracy: 11.20%\n","Epoch 3/50, Loss: 21.3813, Accuracy: 11.25%\n","Epoch 4/50, Loss: 20.7060, Accuracy: 11.29%\n","Epoch 5/50, Loss: 20.2107, Accuracy: 11.31%\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-bc86ca1dc5ef>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["#since learning was slow decided to experiment with residual connections\n","LR = 0.001  # Learning rate\n","drop_out = 0.2  # Dropout rate\n","batch_dim = 64  # Batch size\n","nn_epochs = 50  # Number of epochs\n","loss_fn = nn.CrossEntropyLoss()  # Cross-entropy loss function for classification\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class ProteinCNN(nn.Module):\n","    def __init__(self, amino_acid_residues, num_classes, drop_out=0.1):\n","        super(ProteinCNN, self).__init__()\n","\n","        # First convolutional block with residual connection\n","        self.conv1a = nn.Conv1d(amino_acid_residues, 256, kernel_size=11, padding=5)\n","        self.bn1a = nn.BatchNorm1d(256)\n","        self.conv1b = nn.Conv1d(256, 256, kernel_size=11, padding=5)\n","        self.bn1b = nn.BatchNorm1d(256)\n","        self.dropout1 = nn.Dropout(drop_out)\n","        self.shortcut1 = nn.Conv1d(amino_acid_residues, 256, kernel_size=1)  # Shortcut layer\n","\n","        # Second convolutional block with residual connection\n","        self.conv2a = nn.Conv1d(256, 128, kernel_size=11, padding=5)\n","        self.bn2a = nn.BatchNorm1d(128)\n","        self.conv2b = nn.Conv1d(128, 128, kernel_size=11, padding=5)\n","        self.bn2b = nn.BatchNorm1d(128)\n","        self.dropout2 = nn.Dropout(drop_out)\n","        self.shortcut2 = nn.Conv1d(256, 128, kernel_size=1)  # Shortcut layer\n","\n","        # Third convolutional block with residual connection\n","        self.conv3a = nn.Conv1d(128, 64, kernel_size=11, padding=5)\n","        self.bn3a = nn.BatchNorm1d(64)\n","        self.conv3b = nn.Conv1d(64, 64, kernel_size=11, padding=5)\n","        self.bn3b = nn.BatchNorm1d(64)\n","        self.dropout3 = nn.Dropout(drop_out)\n","        self.shortcut3 = nn.Conv1d(128, 64, kernel_size=1)  # Shortcut layer\n","\n","        # Output layer\n","        self.conv_out = nn.Conv1d(64, num_classes, kernel_size=11, padding=5)\n","\n","    def forward(self, x):\n","        # First residual block\n","        x = x.permute(0, 2, 1)\n","        residual = self.shortcut1(x)\n","        x = F.relu(self.bn1a(self.conv1a(x)))\n","        x = self.dropout1(x)\n","        x = self.bn1b(self.conv1b(x))\n","        x += residual\n","        x = F.relu(x)\n","\n","        # Second residual block\n","        residual = self.shortcut2(x)\n","        x = F.relu(self.bn2a(self.conv2a(x)))\n","        x = self.dropout2(x)\n","        x = self.bn2b(self.conv2b(x))\n","        x += residual\n","        x = F.relu(x)\n","\n","        # Third residual block\n","        residual = self.shortcut3(x)\n","        x = F.relu(self.bn3a(self.conv3a(x)))\n","        x = self.dropout3(x)\n","        x = self.bn3b(self.conv3b(x))\n","        x += residual\n","        x = F.relu(x)\n","\n","        # Output layer\n","        x = self.conv_out(x)\n","        return x\n","# Instantiate the model\n","model = ProteinCNN(amino_acid_residues=21, num_classes=8, drop_out=0.2)\n","\n","# Optimizer and Learning Rate\n","optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n","\n","# Print the model summary\n","print(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3iLE_6oOFe99","executionInfo":{"status":"ok","timestamp":1733940752448,"user_tz":480,"elapsed":176,"user":{"displayName":"Aditi Surendra Tarate","userId":"05608860842351370008"}},"outputId":"35a0bebf-d7a9-4dd8-f601-95478805029b"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["ProteinCNN(\n","  (conv1a): Conv1d(21, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n","  (bn1a): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (conv1b): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n","  (bn1b): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (dropout1): Dropout(p=0.2, inplace=False)\n","  (shortcut1): Conv1d(21, 256, kernel_size=(1,), stride=(1,))\n","  (conv2a): Conv1d(256, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n","  (bn2a): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (conv2b): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n","  (bn2b): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (dropout2): Dropout(p=0.2, inplace=False)\n","  (shortcut2): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n","  (conv3a): Conv1d(128, 64, kernel_size=(11,), stride=(1,), padding=(5,))\n","  (bn3a): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (conv3b): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))\n","  (bn3b): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (dropout3): Dropout(p=0.2, inplace=False)\n","  (shortcut3): Conv1d(128, 64, kernel_size=(1,), stride=(1,))\n","  (conv_out): Conv1d(64, 8, kernel_size=(11,), stride=(1,), padding=(5,))\n",")\n"]}]},{"cell_type":"code","source":["# Training loop\n","train_accuracies = []\n","val_accuracies = []\n","train_losses = []\n","val_losses = []\n","\n","for epoch in range(nn_epochs):\n","    model.train()  # Set model to training mode\n","    train_loss = 0.0\n","    train_correct = 0\n","    train_total = 0\n","\n","    for batch_X, batch_y, mask in train_loader:\n","        optimizer.zero_grad()  # Clear gradients\n","\n","        # Forward pass\n","        #batch_X = batch_X.permute(0, 2, 1)\n","        outputs = model(batch_X)  # Shape: (batch_size, sequence_length, num_classes)\n","        #print(\"Outputs shape before reshaping:\", outputs.shape)\n","\n","        # If batch_y is one-hot encoded, convert to class indices\n","        if batch_y.dim() == 3:  # Check if labels are one-hot\n","          batch_y = batch_y.argmax(dim=2)  # Shape: (batch_size, sequence_length)\n","\n","        # Reshape outputs and labels for CrossEntropyLoss\n","        outputs = outputs.reshape(-1, num_classes)  # Flatten to (batch_size * sequence_length, num_classes)\n","        batch_y = batch_y.reshape(-1)  # Flatten to (batch_size * sequence_length)\n","        mask = mask.reshape(-1)  # (batch_size * sequence_length)\n","\n","        # Apply the mask to exclude padding positions (mask should be 1 for valid, 0 for padding)\n","        outputs_masked = outputs[mask]  # Select valid positions\n","        batch_y_masked = batch_y[mask]  # Select valid labels\n","\n","        #print(\"Labels shape after reshaping:\", batch_y.shape)\n","        #print(\"Outputs shape after reshaping:\", outputs.shape)\n","\n","        assert outputs.shape[0] == batch_y.shape[0], \"Mismatch in output and label batch sizes\"\n","        # Compute loss\n","        loss = loss_fn(outputs_masked, batch_y_masked)\n","        train_loss += loss.item()\n","\n","        # Backward pass and optimization\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Compute accuracy\n","\n","        #mask = (batch_y != padding_label)  # Mask to exclude padding\n","        _, predicted = torch.max(outputs_masked, dim=1)  # Predicted class indices\n","        train_correct += (predicted == batch_y_masked).sum().item()\n","        train_total += mask.sum().item() # Count valid residues\n","    # Calculate epoch-level metrics\n","    train_accuracy = 100 * train_correct / train_total\n","    train_loss /= len(train_loader)\n","\n","    # Validation loop\n","    model.eval()  # Set model to evaluation mode\n","    val_loss = 0.0\n","    val_correct = 0\n","    val_total = 0\n","\n","    with torch.no_grad():\n","        for batch_X, batch_y, mask in val_loader:\n","            #batch_X = batch_X.permute(0, 2, 1)\n","            outputs = model(batch_X)\n","            if batch_y.dim() == 3:  # Check if labels are one-hot\n","              batch_y = batch_y.argmax(dim=2)\n","            # Reshape for loss computation\n","            outputs = outputs.reshape(-1, num_classes)\n","            batch_y = batch_y.reshape(-1)\n","            mask = mask.reshape(-1)\n","\n","            # Apply the mask to exclude padding positions\n","            outputs_masked = outputs[mask]\n","            batch_y_masked = batch_y[mask]\n","\n","            loss = loss_fn(outputs_masked, batch_y_masked)\n","            val_loss += loss.item()\n","\n","            # Compute accuracy\n","            _, predicted = torch.max(outputs_masked, dim=1)\n","            val_correct += (predicted == batch_y_masked).sum().item()\n","            val_total += mask.sum().item()  # Count valid residues\n","\n","    # Calculate validation metrics\n","    val_accuracy = 100 * val_correct / val_total\n","    val_loss /= len(val_loader)\n","\n","    train_accuracies.append(train_accuracy)\n","    val_accuracies.append(val_accuracy)\n","    train_losses.append(train_loss)\n","    val_losses.append(val_loss)\n","\n","    print(f\"Epoch [{epoch+1}/{nn_epochs}]\")\n","    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n","    print(f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"lLolwLBVKAp7","outputId":"233899b3-dd28-4d5d-feca-05a8d6af03ef","executionInfo":{"status":"error","timestamp":1733952856679,"user_tz":480,"elapsed":1862569,"user":{"displayName":"Aditi Surendra Tarate","userId":"05608860842351370008"}}},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/50]\n","Train Loss: 2.0667, Train Accuracy: 13.69%\n","Val Loss: 2.0539, Val Accuracy: 16.39%\n","Epoch [2/50]\n","Train Loss: 2.0473, Train Accuracy: 14.54%\n","Val Loss: 2.0424, Val Accuracy: 17.37%\n","Epoch [3/50]\n","Train Loss: 2.0386, Train Accuracy: 14.86%\n","Val Loss: 2.0355, Val Accuracy: 17.45%\n","Epoch [4/50]\n","Train Loss: 2.0347, Train Accuracy: 15.06%\n","Val Loss: 2.0339, Val Accuracy: 17.17%\n","Epoch [5/50]\n","Train Loss: 2.0326, Train Accuracy: 15.15%\n","Val Loss: 2.0328, Val Accuracy: 17.43%\n","Epoch [6/50]\n","Train Loss: 2.0311, Train Accuracy: 15.31%\n","Val Loss: 2.0320, Val Accuracy: 17.12%\n","Epoch [7/50]\n","Train Loss: 2.0305, Train Accuracy: 15.38%\n","Val Loss: 2.0344, Val Accuracy: 17.25%\n","Epoch [8/50]\n","Train Loss: 2.0301, Train Accuracy: 15.51%\n","Val Loss: 2.0330, Val Accuracy: 17.10%\n","Epoch [9/50]\n","Train Loss: 2.0277, Train Accuracy: 15.71%\n","Val Loss: 2.0318, Val Accuracy: 17.51%\n","Epoch [10/50]\n","Train Loss: 2.0246, Train Accuracy: 15.97%\n","Val Loss: 2.0336, Val Accuracy: 17.14%\n","Epoch [11/50]\n","Train Loss: 2.0229, Train Accuracy: 16.25%\n","Val Loss: 2.0331, Val Accuracy: 17.53%\n","Epoch [12/50]\n","Train Loss: 2.0193, Train Accuracy: 16.59%\n","Val Loss: 2.0335, Val Accuracy: 17.60%\n","Epoch [13/50]\n","Train Loss: 2.0145, Train Accuracy: 16.92%\n","Val Loss: 2.0348, Val Accuracy: 17.47%\n","Epoch [14/50]\n","Train Loss: 2.0108, Train Accuracy: 17.30%\n","Val Loss: 2.0343, Val Accuracy: 17.43%\n","Epoch [15/50]\n","Train Loss: 2.0047, Train Accuracy: 17.85%\n","Val Loss: 2.0401, Val Accuracy: 17.51%\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-a81e0b669b3e>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m#batch_X = batch_X.permute(0, 2, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_X\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Shape: (batch_size, sequence_length, num_classes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;31m#print(\"Outputs shape before reshaping:\", outputs.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-17-15bcfd4dcde1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshortcut1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1a\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1a\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \"\"\"\n\u001b[0;32m--> 193\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2810\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2812\u001b[0;31m     return torch.batch_norm(\n\u001b[0m\u001b[1;32m   2813\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2814\u001b[0m         \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}